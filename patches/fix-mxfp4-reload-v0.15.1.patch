diff --git a/vllm/model_executor/layers/quantization/mxfp4.py b/vllm/model_executor/layers/quantization/mxfp4.py
--- a/vllm/model_executor/layers/quantization/mxfp4.py
+++ b/vllm/model_executor/layers/quantization/mxfp4.py
@@ -50,7 +50,7 @@
     get_padding_alignment,
 )
 from vllm.model_executor.layers.quantization.utils.quant_utils import is_layer_skipped
-from vllm.model_executor.utils import set_weight_attrs
+from vllm.model_executor.utils import replace_parameter, set_weight_attrs
 from vllm.platforms import current_platform
 from vllm.scalar_type import scalar_types
 from vllm.utils.flashinfer import has_flashinfer
@@ -602,18 +602,18 @@
                 .view(torch.float8_e4m3fn)
             )

-            layer.w13_weight = Parameter(w13_weight, requires_grad=False)
-            layer.w13_weight_scale = Parameter(w13_weight_scale, requires_grad=False)
-            layer.w2_weight = Parameter(w2_weight, requires_grad=False)
-            layer.w2_weight_scale = Parameter(w2_weight_scale, requires_grad=False)
-            layer.w13_bias = Parameter(
-                torch.stack(gemm1_bias_shuffled).reshape(self.num_experts, -1),
-                requires_grad=False,
-            )
-            layer.w2_bias = Parameter(
-                torch.stack(gemm2_bias_shuffled).reshape(self.num_experts, -1),
-                requires_grad=False,
-            )
+            replace_parameter(layer, "w13_weight", w13_weight)
+            replace_parameter(layer, "w13_weight_scale", w13_weight_scale)
+            replace_parameter(layer, "w2_weight", w2_weight)
+            replace_parameter(layer, "w2_weight_scale", w2_weight_scale)
+            replace_parameter(
+                layer, "w13_bias",
+                torch.stack(gemm1_bias_shuffled).reshape(self.num_experts, -1),
+            )
+            replace_parameter(
+                layer, "w2_bias",
+                torch.stack(gemm2_bias_shuffled).reshape(self.num_experts, -1),
+            )
         elif (
             self.mxfp4_backend == Mxfp4Backend.SM100_FI_MXFP4_MXFP8_CUTLASS
             or self.mxfp4_backend == Mxfp4Backend.SM90_FI_MXFP4_BF16
@@ -702,14 +702,10 @@
                 w2_scale_interleaved = block_scale_interleave(
                     w2_s.view(torch.uint8)
                 ).reshape(orig_shape)
-
-                layer.w13_weight = Parameter(w13_weight_swapped, requires_grad=False)
-                layer.w13_weight_scale = Parameter(
-                    w13_scale_interleaved, requires_grad=False
-                )
-                layer.w13_bias = Parameter(w13_bias_swapped, requires_grad=False)
-                layer.w2_weight_scale = Parameter(
-                    w2_scale_interleaved, requires_grad=False
-                )
+                replace_parameter(layer, "w13_weight", w13_weight_swapped)
+                replace_parameter(layer, "w13_weight_scale", w13_scale_interleaved)
+                replace_parameter(layer, "w13_bias", w13_bias_swapped)
+                replace_parameter(layer, "w2_weight_scale", w2_scale_interleaved)
             elif self.mxfp4_backend == Mxfp4Backend.SM90_FI_MXFP4_BF16:

                 def _interleave_mxfp4_cutlass_sm90(w):
@@ -730,18 +726,16 @@
                 w2_scales = w2_weight_scale.to(torch.uint8).view(torch.uint8)
                 w2_scales_interleaved = _interleave_mxfp4_cutlass_sm90(w2_scales)

-                layer.w13_weight = torch.nn.Parameter(
-                    torch.cat([w3_w, w1_w], dim=1), requires_grad=False
-                )
-                layer.w13_bias = torch.nn.Parameter(
-                    w13_bias_swapped, requires_grad=False
-                )
-                layer.w13_weight_scale = torch.nn.Parameter(
-                    w31_scales_interleaved, requires_grad=False
-                )
-                layer.w2_weight_scale = torch.nn.Parameter(
-                    w2_scales_interleaved, requires_grad=False
-                )
+                replace_parameter(
+                    layer, "w13_weight",
+                    torch.cat([w3_w, w1_w], dim=1),
+                )
+                replace_parameter(layer, "w13_bias", w13_bias_swapped)
+                replace_parameter(
+                    layer, "w13_weight_scale", w31_scales_interleaved,
+                )
+                replace_parameter(
+                    layer, "w2_weight_scale", w2_scales_interleaved,
+                )
         elif self.mxfp4_backend == Mxfp4Backend.TRITON:
             from triton_kernels.matmul_ogs import FlexCtx, PrecisionConfig

@@ -749,8 +743,8 @@
             w13_bias = layer.w13_bias.to(torch.float32)
             w2_bias = layer.w2_bias.to(torch.float32)

-            layer.w13_bias = Parameter(w13_bias, requires_grad=False)
-            layer.w2_bias = Parameter(w2_bias, requires_grad=False)
+            replace_parameter(layer, "w13_bias", w13_bias)
+            replace_parameter(layer, "w2_bias", w2_bias)

             # Ideally we'd use FusedMoEModularKernel.prepare_finalize object
             # (stored in self.fused_experts) to determine if the MoE has a
@@ -776,12 +770,10 @@
             self.w2_precision_config = PrecisionConfig(
                 weight_scale=w2_scale, flex_ctx=FlexCtx(rhs_data=w2_flex)
             )

             self.w13_weight = w13_weight
             self.w2_weight = w2_weight
-            del layer.w13_weight
-            del layer.w2_weight
-            layer.w13_weight = w13_weight
-            layer.w2_weight = w2_weight
+            replace_parameter(layer, "w13_weight", w13_weight)
+            replace_parameter(layer, "w2_weight", w2_weight)
         else:
             raise ValueError(
                 f"Unsupported mxfp4_backend: {self.mxfp4_backend}: "
diff --git a/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py b/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
--- a/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
+++ b/vllm/model_executor/layers/quantization/utils/marlin_utils_fp4.py
@@ -6,6 +6,7 @@

 import vllm._custom_ops as ops
 from vllm.logger import init_logger
+from vllm.model_executor.utils import replace_parameter
 from vllm.model_executor.layers.quantization.utils.marlin_utils import (
     USE_FP32_REDUCE_DEFAULT,
     get_marlin_input_dtype,
@@ -386,9 +387,7 @@ def prepare_moe_fp4_layer_for_marlin(
             tensor_list.append(marlin_qweight)

         weight = torch.cat([x.unsqueeze(0) for x in tensor_list], 0)
-        weight = torch.nn.Parameter(weight, requires_grad=False)
-
-        setattr(layer, name, weight)
+        replace_parameter(layer, name, weight)

     # WEIGHT SCALES
     # Permute scales
@@ -425,13 +424,11 @@ def prepare_moe_fp4_layer_for_marlin(
             tensor_list.append(marlin_scales)

         scales = torch.cat([x.unsqueeze(0) for x in tensor_list], 0)
-        scales = torch.nn.Parameter(scales, requires_grad=False)
-        setattr(layer, name + "_weight_scale", scales)
+        replace_parameter(layer, name + "_weight_scale", scales)

         if is_nvfp4:
             global_scale = nvfp4_marlin_process_global_scale(global_scale)
-            global_scale = torch.nn.Parameter(global_scale, requires_grad=False)
-            setattr(layer, name + "_weight_scale_2", global_scale)
+            replace_parameter(layer, name + "_weight_scale_2", global_scale)

     # BIAS
     # Permute bias
@@ -447,8 +444,7 @@ def prepare_moe_fp4_layer_for_marlin(
             tensor_list.append(marlin_permute_bias(expert_bias))

         bias = torch.cat([x.unsqueeze(0) for x in tensor_list], 0)
-        bias = torch.nn.Parameter(bias, requires_grad=False)
-        setattr(layer, name, bias)
+        replace_parameter(layer, name, bias)


 def rand_marlin_weight_nvfp4_like(weight, group_size, input_dtype=None):
