--- a/vllm/model_executor/utils.py
+++ b/vllm/model_executor/utils.py
@@ -60,14 +60,22 @@ def replace_parameter(
     # should not be used on a tied/shared param

     # If new_data is None, set the parameter to None
     if new_data is None:
         setattr(layer, param_name, None)
         return

     if isinstance(new_data, torch.nn.Parameter):
         new_data = new_data.data
     new_param = torch.nn.Parameter(new_data, requires_grad=False)

     old_param: torch.nn.Parameter | None = getattr(layer, param_name, None)
-    if old_param is not None and hasattr(old_param, "weight_loader"):
-        weight_loader = old_param.weight_loader
-        set_weight_attrs(new_param, {"weight_loader": weight_loader})
+    if old_param is not None:
+        # Preserve the parameter subclass (e.g. RowvLLMParameter,
+        # ModelWeightParameter) so that methods like
+        # load_row_parallel_weight survive reload_weights.
+        new_param.__class__ = old_param.__class__
+        # Copy all instance attributes (weight_loader, output_dim, etc.)
+        # from the old parameter to the new one.
+        for attr, value in old_param.__dict__.items():
+            if attr not in ("data",):
+                setattr(new_param, attr, value)

     setattr(layer, param_name, new_param)
